{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "potential-enemy",
   "metadata": {},
   "source": [
    "# Glaciated basin runoff analysis\n",
    "\n",
    "This will work as the prototype for how to select a glaciated basin, simulate its glaciers and analyse the outputs. Ideally this will be made into a module later for simpler use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as shpg\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by downloading the rgi data\n",
    "from oggm import utils\n",
    "utils.get_rgi_dir(version='62')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-shipping",
   "metadata": {},
   "source": [
    "## Access a region\n",
    "![rgi-map](https://www.researchgate.net/profile/Tobias_Bolch/publication/264125572/figure/fig1/AS:295867740377088@1447551774164/First-order-regions-of-the-RGI-with-glaciers-shown-in-red-Region-numbers-are-those-of.png)\n",
    "*Source: [the RGI consortium](http://www.glims.org/RGI/randolph60.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fetches Central Europe\n",
    "fr = utils.get_rgi_region_file('11', version='62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select glaciers per attribute\n",
    "gdf_sel = gdf.loc[gdf.O2Region == '2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-cooperative",
   "metadata": {},
   "source": [
    "## Selecting glaciers in a basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = utils.get_demo_file('rofental_hydrosheds.shp')\n",
    "basin = gpd.read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-exemption",
   "metadata": {},
   "source": [
    "We can select all glaciers that lies within a shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_glaciers(basin, gdf):\n",
    "    '''Function to select the glaciers within a basin.\n",
    "    -----\n",
    "    arguments:\n",
    "    basin: geopandas dataframe of the basin (one shapefile)\n",
    "    gdf: geopandas dataframe containing the glaciers of the\n",
    "    region.\n",
    "    \n",
    "    returns:\n",
    "    geopandas dataframe of the glaciers within the basin.\n",
    "    \n",
    "    '''\n",
    "    in_bas = [basin.geometry.contains(shpg.Point(x, y))[0]\n",
    "              for (x, y) in zip(gdf.CenLon, gdf.CenLat)]\n",
    "    return gdf.loc[in_bas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = basin.plot();\n",
    "gdf_sel = select_glaciers(basin, gdf)\n",
    "gdf_sel.plot(ax=ax, edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-contamination",
   "metadata": {},
   "source": [
    "This gives us the RGIIDs which can be used to initilaize gdirs for simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm import cfg, workflow, tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initial cfg.\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['continue_on_error'] = True\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "cfg.PARAMS['border'] = 80\n",
    "# Set the path\n",
    "cfg.PATHS['working_dir'] = '/home/erik/data/oggm_output/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.4/L3-L5_files/CRU/elev_bands/qc3/pcp2.5/no_match'\n",
    "# # Initialize the gdirs\n",
    "# gdirs = workflow.init_glacier_directories(gdf_sel,\n",
    "#                                           from_prepro_level=5,\n",
    "#                                           prepro_border=80,\n",
    "#                                           prepro_base_url=base_url )\n",
    "gdirs = workflow.init_glacier_directories(gdf_sel.RGIId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-affiliate",
   "metadata": {},
   "source": [
    "Now we can get the climate data and process it for our glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oggm.shop import gcm_climate\n",
    "bp = 'https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/pr/pr_mon_CCSM4_{}_r1i1p1_g025.nc'\n",
    "bt = 'https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/tas/tas_mon_CCSM4_{}_r1i1p1_g025.nc'\n",
    "for rcp in ['rcp26', 'rcp45', 'rcp60', 'rcp85']:\n",
    "    # Download the files\n",
    "    ft = utils.file_downloader(bt.format(rcp))\n",
    "    fp = utils.file_downloader(bp.format(rcp))\n",
    "    # bias correct them\n",
    "    workflow.execute_entity_task(gcm_climate.process_cmip_data, gdirs,\n",
    "                                 # recognize the climate file for later\n",
    "                                 filesuffix='_CCSM4_{}'.format(rcp),\n",
    "                                 # temperature projections\n",
    "                                 fpath_temp=ft,\n",
    "                                 # precip projections\n",
    "                                 fpath_precip=fp,\n",
    "                                 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hydro_projections(gdirs, rcps):\n",
    "    '''Small wrapper for running hydro simulations\n",
    "    arguments:\n",
    "    gdirs: glacier directories.\n",
    "    rcps: list of rcp scenarios to run.\n",
    "    '''\n",
    "    for rcp in rcps:\n",
    "        rid = f'_CCSM4_{rcp}'\n",
    "        workflow.execute_entity_task(\n",
    "                             tasks.run_with_hydro,  gdirs,\n",
    "                             run_task=tasks.run_from_climate_data,\n",
    "                             ys=2020,\n",
    "                             # Use gcm_data\n",
    "                             climate_filename='gcm_data',\n",
    "                             # Use the scenario\n",
    "                             climate_input_filesuffix=rid,\n",
    "                             # When to start?\n",
    "                             init_model_filesuffix='_historical',\n",
    "                             # Good naming for recognizing later\n",
    "                             output_filesuffix=rid,\n",
    "                             # Store monthyl?\n",
    "                             store_monthly_hydro=True,\n",
    "                            )\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and simulate everything for one rcp.\n",
    "rcps = ['rcp26']\n",
    "run_hydro_projections(gdirs, rcps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already have the dataset. Load it below\n",
    "# output_suffix = '_rofental_CCMSM4_rcp26'\n",
    "# ds = utils.compile_run_output(gdirs,\n",
    "#                               input_filesuffix='_CCSM4_rcp26',\n",
    "#                               output_filesuffix=output_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpth = cfg.PATHS['working_dir'] + 'run_output_rofental_CCMSM4_rcp26.nc'\n",
    "with xr.open_dataset(fpth) as ds:\n",
    "       ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_vars = [v for v in ds.variables if 'month_2d' not in ds[v].dims]\n",
    "df_annual = ds[sel_vars].to_dataframe()\n",
    "runoff_vars = ['melt_off_glacier', 'melt_on_glacier',\n",
    "               'liq_prcp_off_glacier', 'liq_prcp_on_glacier']\n",
    "df_runoff = df_annual[runoff_vars].clip(0) * 1e-9\n",
    "df_runoff = df_runoff.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18, 7))\n",
    "df_runoff.sum(axis=1).rolling(window=11).mean().plot(ax=ax)\n",
    "ax.set_ylabel('Annual runoff (Mt)')\n",
    "ax.set_xlabel('Year')\n",
    "plt.title('Annual runoff from glaciers in Rofental under RCP2.6');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-regulation",
   "metadata": {},
   "source": [
    "## Prototyping the runoff in Rofental\n",
    "\n",
    "**What to do in this part**\n",
    "- [ ] ~~Figure out how to divide a basin into subparts.~~\n",
    "- [ ] Downscale temperature and precipitation to the individual subparts.\n",
    "- [ ] Calculate the total precipitation falling onto a subpart. Make use of the area and previously calculated precipitation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the basin.\n",
    "basin.plot();\n",
    "plt.title('Rofental');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-validity",
   "metadata": {},
   "source": [
    "### Get the precipitation for the basin\n",
    "\n",
    "Currently just using one of the gcm datasets for one of the glaciers. This should be for the basin in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we are just using the climate for one of the glaciers as\n",
    "# a proof of concept. Should do downscaling on the basin later?\n",
    "cl_path = gdirs[0].get_filepath(filename='gcm_data',\n",
    "                                filesuffix='_CCSM4_rcp26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(cl_path) as ds:\n",
    "    clim_ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_ds.sel(time=slice('2020', '2100')).prcp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-brazilian",
   "metadata": {},
   "source": [
    "Calculate the area of the basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_test = basin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area in sqm.\n",
    "basin_area = basin_test.to_crs({'proj': 'cea'}).area[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-torture",
   "metadata": {},
   "source": [
    "Calculate the glacier covered area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rgi area of each glacier. Maybe get this from diagnostics?\n",
    "# but we only want the largest extent anyway since hydro in OGGM\n",
    "# is calculated on this.\n",
    "gl_area_list = [gdir.rgi_area_m2 for gdir in gdirs]\n",
    "glacier_area = sum(gl_area_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-pixel",
   "metadata": {},
   "source": [
    "### Calculate the total precipitation\n",
    "\n",
    "Here we calculate the total monthly precipitation. It is just the basin area times the precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now basically ready to calculate the precipitation with and without glaciers. I.e. p_tilde in Utlee and Coata. But start witht the\n",
    "# \"normal\" precipitation.\n",
    "prcp = clim_ds.prcp\n",
    "prcp = prcp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp.plot();\n",
    "plt.title('Moisture availabiilty in Rofental');\n",
    "plt.ylabel('Monthly precipitaion [kg m$^{-2}$]');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-greenhouse",
   "metadata": {},
   "source": [
    "And calculating the available water with glacier runoff included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ice free area in sqm. A - A_g\n",
    "basin_ice_free_area = basin_area - glacier_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-claim",
   "metadata": {},
   "source": [
    "Lets open the compiled model diagnostics from our run, this contains the total runoff from glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpth = cfg.PATHS['working_dir'] + 'run_output_rofental_CCMSM4_rcp26.nc'\n",
    "fpth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(fpth) as ds:\n",
    "    glacier_proj = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-starter",
   "metadata": {},
   "source": [
    "Get the monthly runoff. Has to move the timeseries to account for the hydrological year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should work in both hemispheres maybe?\n",
    "ds_roll = glacier_proj.roll(month_2d=ds['calendar_month_2d'].data[0] - 1,\n",
    "                            roll_coords=True)\n",
    "ds_roll['month_2d'] = ds_roll['calendar_month_2d']\n",
    "\n",
    "# Select only the runoff variables\n",
    "monthly_runoff = (ds_roll['melt_off_glacier_monthly'] +\n",
    "                  ds_roll['melt_on_glacier_monthly'] +\n",
    "                  ds_roll['liq_prcp_off_glacier_monthly'] +\n",
    "                  ds_roll['liq_prcp_on_glacier_monthly'])\n",
    "# Sum all the glaciers and convert it to kg m-2 (mm), i.e. divide by the\n",
    "# glacier area.\n",
    "monthly_runoff = monthly_runoff.sum(dim='rgi_id') / glacier_area\n",
    "# clip runoff to 0.\n",
    "monthly_runoff = monthly_runoff.clip(0)\n",
    "\n",
    "# Get the values flat, no special indexing. Create a pd dataframe and index\n",
    "# from 2021.\n",
    "monthly_runoff = pd.DataFrame(monthly_runoff.values.flatten(),\n",
    "                              columns=['melt'],\n",
    "                              index=pd.date_range('2020', '2100-12-01',\n",
    "                                                  freq='MS'))\n",
    "\n",
    "# Shift the index 14 days to the middle of the month. \n",
    "monthly_runoff = monthly_runoff.shift(14, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_runoff.plot()\n",
    "plt.ylabel('Monthly runoff [kg m$^{-2}$]')\n",
    "plt.title('Glaicier runoff in Rofental');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-consumption",
   "metadata": {},
   "source": [
    "Now we have monthly runoff from the glaciers. With this we can calculate the \"alternative\" moisture availability. Lets put the precipitation and melt in the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "rofental_hydro = pd.concat([prcp.loc['2020':], monthly_runoff.iloc[:-4]],\n",
    "                           axis=1)\n",
    "# Create the adjusted precipitation. I.e. precipitation per sqm on ice free\n",
    "# area.\n",
    "rofental_hydro['prcp_adj'] = (basin_ice_free_area/basin_area)\\\n",
    "* rofental_hydro['prcp']\n",
    "# Melt ajd\n",
    "rofental_hydro['melt_adj'] = (glacier_area / basin_area) *\\\n",
    "                              rofental_hydro['melt']\n",
    "# adjusted moisture availability\n",
    "rofental_hydro['moisture_adj'] = rofental_hydro[['prcp_adj',\n",
    "                                                 'melt_adj']].sum(axis=1)\n",
    "# Print the df\n",
    "rofental_hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "rofental_hydro['moisture_adj'].rolling(15).mean().plot(ax=ax,\n",
    "                                                       label='Precip. +'+\n",
    "                                                       ' glaciers')\n",
    "rofental_hydro['prcp'].rolling(15).mean().plot(ax=ax, label='Precip. only')\n",
    "rofental_hydro['melt_adj'].rolling(15).mean().plot(ax=ax, label='Glaciers only')\n",
    "plt.title('Water availability in Rofental under RCP2.6')\n",
    "plt.ylabel('Monthly runoff [kg m$^{-2}$]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-burning",
   "metadata": {},
   "source": [
    "A first prototype plot exploring the water availability in Rofental. It showcases how the inclusion of glacier melt buffers water availability during periods of drier conditoins (i.e. 2050 and ~2070). Note that plotted is a 15 month running mean.\n",
    "\n",
    "Glacier melt is calculated using OGGM while the precipitation comes from CMIP5 data(?). The combined precipitation and glacier runoff is calculated following Utlee and Coats\n",
    "$$\n",
    "\\tilde{p} = \\frac{A-A_g}{A}p + \\frac{A_g}{A}r\n",
    "$$\n",
    "This assumes that both precipitation and melt has the unit kg m$^{-2}$. $r$ is scaled to the glaciated area, and thus has to be scaled to the whole basin are to give the correct measurement. This is why the adjusted melt is much lower than the original.\n",
    "\n",
    "To continue we need to caclualte PET. This can be done in a multitude of ways. Since we will eventually do this on many different basins (data availability might be scarce) the Thornthwaite method will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-stick",
   "metadata": {},
   "source": [
    "## Calculate PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will create a function for this.\n",
    "def calc_PET(clim_ds, future=True):\n",
    "    '''Function used to calculate PET. PET has the unit of mm/month.''' \n",
    "    \n",
    "    if future:\n",
    "        clim_ds = clim_ds.sel(time=slice('2020', '2100'))\n",
    "    else:\n",
    "        clim_ds = clim_ds.sel(time=slice('1910', '2010'))\n",
    "    # Get the latitude from the ds\n",
    "    latitude = clim_ds.attrs['ref_pix_lat']\n",
    "    # Convert the lat to radians\n",
    "    latitude = np.deg2rad(latitude)\n",
    "    # Average Julian day, is this then the average of all the julian dates of \n",
    "    # the month?\n",
    "    time_df = clim_ds.to_dataframe()\n",
    "    # This is not the average yet. But since we are taking the middle day it\n",
    "    # shouldn't matter anyway.\n",
    "    time_df['julian'] = pd.DatetimeIndex(time_df.index).to_julian_date()\n",
    "    # Calculate the days in each month\n",
    "    time_df['days_of_month'] = pd.DatetimeIndex(time_df.index).days_in_month\n",
    "    J = time_df['julian'].to_numpy()\n",
    "    NDM = time_df['days_of_month'].to_numpy()\n",
    "    \n",
    "    # Calc solar declination\n",
    "    solar_declination = 0.4093 * np.sin((2 * np.pi * J / 365) - 1.405)\n",
    "    # Calculate the hourly angle of sun rising\n",
    "    h_sun_angle = np.arccos(-np.tan(latitude) * np.tan(solar_declination))\n",
    "    # Maximum number of sun hours\n",
    "    N = (24 / np.pi) * h_sun_angle\n",
    "    # Correction coefficient\n",
    "    K = (N / 12) * (NDM / 30)\n",
    "    # Heat index\n",
    "    # Should this be a rolling sum?\n",
    "    I = ((clim_ds.temp / 5)**1.514).groupby('time.year').sum().values\n",
    "    # We repeat each value 12 times.\n",
    "    I = np.repeat(I, 12)\n",
    "    if future:\n",
    "        I = I[:-4]\n",
    "    # Get m\n",
    "    m = (6.75 * 1e-7) * I**3 - (7.71 * 1e-5) * I**2 + (1.792 * 1e-2) * I +\\\n",
    "        0.49239\n",
    "    temp = clim_ds.temp.values\n",
    "    # Final PET\n",
    "    PET = np.power(16 * K * (10 * temp / I), m)\n",
    "    return PET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-billion",
   "metadata": {},
   "source": [
    "We can use the new function to calculate PET from a temperature dataset. It is inserted into the `rofental_hydro` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pet from a dataset with temp.\n",
    "pet = calc_PET(clim_ds)\n",
    "rofental_hydro['PET'] = pet\n",
    "rofental_hydro = rofental_hydro.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-polls",
   "metadata": {},
   "source": [
    "And lets calculate the available mositure with and without glacier contribution\n",
    "$$\n",
    "D = P - PET.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to convert the moisture since x kg/m2 = x 1e-3 m = x mm.\n",
    "# Not adjusted\n",
    "rofental_hydro['D'] = rofental_hydro['prcp'] - rofental_hydro['PET']\n",
    "# Adjusted\n",
    "rofental_hydro['D_adj'] = rofental_hydro['moisture_adj'] - rofental_hydro['PET']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-religion",
   "metadata": {},
   "source": [
    "Plot them both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "rofental_hydro['D'].rolling(15).mean().plot(ax=ax, label='Precip. only')\n",
    "rofental_hydro['D_adj'].rolling(15).mean().plot(ax=ax, label='Precip. + '+\n",
    "                                                'glacier')\n",
    "plt.title('Available moisture - PET in Rofental under RCP2.6')\n",
    "plt.ylabel('Mositre [mm]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-dynamics",
   "metadata": {},
   "source": [
    "## SPEI\n",
    "\n",
    "Let's calculate SPEI. Since we are talking about drought we want to look at the accumulated mositure difference $D$ at different time scales. In a given month $j$ during year $i$ the difference $D_{i,j}^k$ depends on the timescale $k$ (12 in this case)\n",
    "$$\n",
    "X^K_{i,j} = \\sum_{l=13-k+j}^{12} D_{i-1, l} + \\sum_{l=1}^j D_{i,l}, \\space \\mathrm{if} \\space j<k,\n",
    "$$\n",
    "$$\n",
    "X^K_{i,j} = \\sum_{l=j-k+j}^j D_{i-1, l}, \\space \\mathrm{if} \\space j\\geq k.\n",
    "$$\n",
    "This is essentially a rolling sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "rofental_hydro['D_adj'].rolling(15).sum().plot(ax=ax);\n",
    "plt.title(\n",
    "    '15 month accumulated moisture availability in Rofental under RCP2.6');\n",
    "plt.ylabel('Moisture [mm]');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-clinic",
   "metadata": {},
   "source": [
    "It is now time for the statistics part to SPEI, normalize it so to say. Use log-logistic distribution according to Vicente-Serrano et. al. But we need an observed D-series (or historical at least) to model the distribution. This distribution is then used to get the probabilities for SPEI. Should I use climate historical for this and the model diagnostics historical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_hist = gdirs[0].get_filepath(filename='climate_historical')\n",
    "with xr.open_dataset(clim_hist) as ds:\n",
    "    clim_hist_ds = ds.isel(time=slice(0, -1)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_hist_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-index",
   "metadata": {},
   "source": [
    "What timeperiod should we go for, 1910 to 2007 as in the paper? Let's be smart and use scipy for this. No need to implement a distribution fit by my self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisk is the name of a log logistic distribution.\n",
    "from scipy.stats import fisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_vars = fisk.fit(clim_hist_ds.prcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "x = np.linspace(0, 300, 100)\n",
    "ax.plot(x, fisk.pdf(x, *fit_vars));\n",
    "clim_hist_ds.prcp.plot.hist(ax=ax, bins=30, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-qatar",
   "metadata": {},
   "source": [
    "That is basically all the parts. Let's put them together to calculate SPEI on the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_hist_df = clim_hist_ds.sel(time=slice('1910', '2010')).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PET and put into df.\n",
    "pet = calc_PET(clim_hist_ds, future=False)\n",
    "clim_hist_df['PET'] = pet\n",
    "clim_hist_df = clim_hist_df.fillna(0)\n",
    "# Calculate D\n",
    "clim_hist_df['D'] = clim_hist_df['prcp'] - clim_hist_df['PET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-offense",
   "metadata": {},
   "source": [
    "Lets take a look at D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "clim_hist_df['D'].rolling(15).sum().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-sunset",
   "metadata": {},
   "source": [
    "## Fit distribution to D series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test it with a 15 month window\n",
    "fit = fisk.fit(clim_hist_df['D'].rolling(15).sum().dropna())\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "x = np.linspace(1000, 2300, 100)\n",
    "ax.plot(x, fisk.pdf(x, *fit), label='Log-logistic')\n",
    "clim_hist_df['D'].rolling(15).sum().dropna().plot.hist(ax=ax,\n",
    "                                                       density=True,\n",
    "                                                       bins=30);\n",
    "plt.title('Fit distribution and histogram of prcp. - PET')\n",
    "ax.set_xlabel('Accumulated precipitation [mm]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to calc SPEI\n",
    "def calc_SPEI(d_historical, d_spei, k):\n",
    "    '''Calcualte SPEI, a drought index\n",
    "   \n",
    "    arguments:\n",
    "    d_historical: Pandas series containing historical data for the region.\n",
    "                   Used to fit the log-logistic distribution.\n",
    "    d_spei: Pandas series containing data on which SPEI should be calculated.\n",
    "    \n",
    "    returns:\n",
    "    spei: A pandas dataframe with SPEI values.\n",
    "    ''' \n",
    "    # Start with calculating the rolling sum of desired length.\n",
    "    history = d_historical.rolling(k).sum().dropna()\n",
    "    # Fit the fisk distribution.\n",
    "    fit = fisk.fit(history)\n",
    "    \n",
    "    # Constants\n",
    "    C0 = 2.515517\n",
    "    C1 = 0.802853\n",
    "    C2 = 0.010328\n",
    "    d1 = 1.432788\n",
    "    d2 = 0.1819269\n",
    "    d3 = 0.001308\n",
    "    # Calculate the rolling sum of the invsetigated runoff.\n",
    "    D = d_spei.rolling(k).sum().dropna()\n",
    "    # Calc P, use the cdf even if the paprt says probability density function.\n",
    "    P = 1 - fisk.cdf(D, *fit)\n",
    "    # Calc W for P <= 0.5\n",
    "    W = np.where(P < 0.5, np.sqrt(-2 * np.log(P)),\n",
    "                 -np.sqrt(-2 * np.log(1-P)))\n",
    "    # Calc SPEI\n",
    "    SPEI = W - (C0 + C1 * W + C2 * W**2) / (1 + d1 * W + d2 * W**2 + d3 * W**3)\n",
    "    SPEI = pd.DataFrame(SPEI, index=D.index, columns=['SPEI'])\n",
    "    return SPEI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-champion",
   "metadata": {},
   "source": [
    "Testing the **SPEI** calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spei = rofental_hydro['D']\n",
    "test_spei_gl = rofental_hydro['D_adj']\n",
    "test_history = clim_hist_df['D']\n",
    "spei = calc_SPEI(test_history, test_spei, 15)\n",
    "spei_adj = calc_SPEI(test_history, test_spei_gl, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "spei['SPEI'].plot(ax=ax, label='Prcp. only')\n",
    "spei_adj['SPEI'].plot(ax=ax, label='Prcp + glacier')\n",
    "ax.set_ylabel('SPEI')\n",
    "plt.title('15 month SPEI in Rofental under RCP2.6');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-reward",
   "metadata": {},
   "source": [
    "But this is a bit **misleading**. Not nice to compare runoff including glaciers with  to the historical data which doesn't include glacier runoff. What happens without it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_adj = calc_SPEI(rofental_hydro['D_adj'], rofental_hydro['D_adj'], 15)\n",
    "spei = calc_SPEI(rofental_hydro['D'], rofental_hydro['D'], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "spei_adj['SPEI'].plot(ax=ax, label='Prcp. + glacier')\n",
    "spei['SPEI'].plot(ax=ax, label='Prcp. only')\n",
    "ax.set_ylabel('SPEI')\n",
    "plt.title('15 month SPEI in Rofental under RCP2.6');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-mistake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oggm",
   "language": "python",
   "name": "oggm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
